{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,class_likelihood_ratios\n",
    "import pydot # it needs Graphviz to be installed as well\n",
    "import clean\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables used for the creation of the corpus and the configuration of the model\n",
    "ProjConfig = {\n",
    "  'under_sample': True, #use undersampling of Shakespeare corpus sentences\n",
    "  'under_sampling_weight': 100, #the number of sentences added to the other author's median\n",
    "  'over_sample': False, #use overersampling for Marlowe corpus sentences\n",
    "  'over_sampling_fraction': 2, #fraction of the corpus used for oversampling\n",
    "  'batch_size': 16,  #number of batches for the creation of datasets\n",
    "  'epochs': 6, #number of epochs for the training of the model\n",
    "  'max_seq_len': 16, #sets the maximum sentence length to a value close to the median sentence length of the corpus (BERT max is 512)\n",
    "  'learning_rate': 4e-5, #sets the learning rate for ADAM optimizer\n",
    "  'decay_factor': 0.0001,\n",
    "  'cycles': 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "if 'COLAB_TPU_ADDR' in os.environ:\n",
    "  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
    "  tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "  strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
    "  print('Using TPU')\n",
    "elif tf.config.list_physical_devices('GPU'):\n",
    "  strategy = tf.distribute.MirroredStrategy()\n",
    "  print('Using GPU')\n",
    "else:\n",
    "  raise ValueError('Running on CPU is not recommended.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importPlaysinCorpus(CorpusPath,Author):\n",
    "    \"\"\"Imports plays in text form to the corpus dataframe (preferable cleaned)\"\"\"\n",
    "    corpus=pd.DataFrame()\n",
    "    for play in os.scandir(CorpusPath):\n",
    "        if play.is_file():\n",
    "            print((\" importing  \") + (play.path) + (\" to corpus\"))\n",
    "            newplay=pd.read_csv(play.path, delimiter='\\r', header=None, names=['sentence', 'author', 'play'])\n",
    "            newplay[['author']]=Author\n",
    "            if 'Cleaned' in play.path:\n",
    "                newplay[['play']]=os.path.basename(play.path).removesuffix('Cleaned.txt')\n",
    "            else:\n",
    "                newplay[['play']]=os.path.basename(play.path).removesuffix('.txt')\n",
    "            corpus=pd.concat([corpus, newplay], axis = 0,join='outer')\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " importing  ./Corpus/Marlowe/Dido.txt to corpus\n",
      " importing  ./Corpus/Marlowe/DrFaustus.txt to corpus\n",
      " importing  ./Corpus/Marlowe/EdwardII.txt to corpus\n",
      " importing  ./Corpus/Marlowe/JewOfMalta.txt to corpus\n",
      " importing  ./Corpus/Marlowe/Tamburlaine1.txt to corpus\n",
      " importing  ./Corpus/Marlowe/Tamburlaine2.txt to corpus\n",
      " importing  ./Corpus/Shakespeare/CleanedPlays/AnthonyCleopatraCleaned.txt to corpus\n",
      " importing  ./Corpus/Shakespeare/CleanedPlays/HenryVIIICleaned.txt to corpus\n",
      " importing  ./Corpus/Shakespeare/CleanedPlays/HenryVCleaned.txt to corpus\n",
      " importing  ./Corpus/Shakespeare/CleanedPlays/MacbethCleaned.txt to corpus\n",
      " importing  ./Corpus/Shakespeare/CleanedPlays/RichardIIICleaned.txt to corpus\n",
      " importing  ./Corpus/Shakespeare/CleanedPlays/HamletCleaned.txt to corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Panos\\AppData\\Local\\Temp\\ipykernel_7652\\3063647115.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  projectCorpus=projectCorpus.append(importPlaysinCorpus(ShakespearePath,'Shakespeare'))\n"
     ]
    }
   ],
   "source": [
    "projectCorpus=pd.DataFrame()\n",
    "MarlowePath='./Corpus/Marlowe/'\n",
    "ShakespearePath='./Corpus/Shakespeare/CleanedPlays/'\n",
    "\n",
    "projectCorpus=importPlaysinCorpus(MarlowePath,'Marlowe')\n",
    "projectCorpus=projectCorpus.append(importPlaysinCorpus(ShakespearePath,'Shakespeare'))\n",
    "projectCorpus['sentence']=projectCorpus['sentence'].str.lower()\n",
    "NewProjectCorpus=projectCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over and under sampling corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ProjConfig['over_sample']==True:\n",
    "    ShakespeareMedian=projectCorpus.loc[projectCorpus['author'] == 'Shakespeare'].play.value_counts().median()\n",
    "    MarloweCorpusAddition=pd.DataFrame()\n",
    "    for play in projectCorpus.loc[projectCorpus['author'] == 'Marlowe'].play.unique():\n",
    "        selectedPlay=projectCorpus.loc[projectCorpus['play'] == play] \n",
    "        if selectedPlay.value_counts().sum() < ShakespeareMedian:\n",
    "            selecteplaySample=selectedPlay.sample(int(selectedPlay['sentence'].value_counts().sum()/ProjConfig['over_sampling_fraction']))\n",
    "            MarloweCorpusAddition=pd.concat([MarloweCorpusAddition,selecteplaySample])\n",
    "            print('adding a sample from ', play, 'to Marlowe\\'s corpus')\n",
    "    #adding a fraction of Marlowe's corpus to increase its volume\n",
    "    NewProjectCorpus=pd.concat([projectCorpus, MarloweCorpusAddition], axis = 0,join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " importing  ./Corpus/Marlowe/Dido.txt to corpus\n",
      " importing  ./Corpus/Marlowe/DrFaustus.txt to corpus\n",
      " importing  ./Corpus/Marlowe/EdwardII.txt to corpus\n",
      " importing  ./Corpus/Marlowe/JewOfMalta.txt to corpus\n",
      " importing  ./Corpus/Marlowe/Tamburlaine1.txt to corpus\n",
      " importing  ./Corpus/Marlowe/Tamburlaine2.txt to corpus\n"
     ]
    }
   ],
   "source": [
    "if ProjConfig['under_sample']==True:\n",
    "    ShakespeareMedian=projectCorpus.loc[projectCorpus['author'] == 'Shakespeare'].play.value_counts().median()\n",
    "    NewShakesepareCorpus=pd.DataFrame()\n",
    "    MarloweMedian=projectCorpus.loc[projectCorpus['author'] == 'Marlowe'].play.value_counts().median()\n",
    "    if MarloweMedian < ShakespeareMedian:\n",
    "        for play in projectCorpus.loc[projectCorpus['author'] == 'Shakespeare'].play.unique():\n",
    "            selectedPlay=projectCorpus.loc[projectCorpus['play'] == play]\n",
    "            if selectedPlay.value_counts().sum() > MarloweMedian:\n",
    "                playSample = selectedPlay.sample(int(MarloweMedian-ProjConfig['under_sampling_weight']))\n",
    "                NewShakesepareCorpus=pd.concat([NewShakesepareCorpus, playSample], axis = 0,join='outer')\n",
    "    #checking if over-sampled texts have been added\n",
    "    if NewProjectCorpus.sentence.count()==projectCorpus.sentence.count():\n",
    "        NewProjectCorpus = importPlaysinCorpus(MarlowePath,'Marlowe')        \n",
    "        NewProjectCorpus = pd.concat([NewProjectCorpus, NewShakesepareCorpus], axis = 0,join='outer')\n",
    "        NewProjectCorpus['sentence']=NewProjectCorpus['sentence'].str.lower()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coprus Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordsPerPlay(corpus):\n",
    "    columns=['Play', 'Words Sum', 'Author']\n",
    "    PlayList=[]\n",
    "    for Play in corpus['play'].unique():\n",
    "            PlayInLoop=(corpus.loc[corpus['play'] == Play])\n",
    "            PlaySum = sum(PlayInLoop['sentence'].str.count('\\w+'))\n",
    "            PlayName = Play\n",
    "            PlayAuthor = PlayInLoop['author'].unique()[0]\n",
    "            PlayList.append([PlayName,PlaySum,PlayAuthor])\n",
    "            \n",
    "    WordsPerPlay=pd.DataFrame(PlayList, columns=columns)\n",
    "    return WordsPerPlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial corpus:\n",
      "                 Play  Words Sum       Author\n",
      "0               Dido      14003      Marlowe\n",
      "1          DrFaustus      12040      Marlowe\n",
      "2           EdwardII      21164      Marlowe\n",
      "3         JewOfMalta      19328      Marlowe\n",
      "4       Tamburlaine1      17666      Marlowe\n",
      "5       Tamburlaine2      17921      Marlowe\n",
      "6   AnthonyCleopatra      24163  Shakespeare\n",
      "7          HenryVIII      24230  Shakespeare\n",
      "8             HenryV      28687  Shakespeare\n",
      "9            Macbeth      16859  Shakespeare\n",
      "10        RichardIII      30886  Shakespeare\n",
      "11            Hamlet      29958  Shakespeare \n",
      " New Corpus:\n",
      "                 Play  Words Sum       Author\n",
      "0               Dido      14003      Marlowe\n",
      "1          DrFaustus      12040      Marlowe\n",
      "2           EdwardII      21164      Marlowe\n",
      "3         JewOfMalta      19328      Marlowe\n",
      "4       Tamburlaine1      17666      Marlowe\n",
      "5       Tamburlaine2      17921      Marlowe\n",
      "6   AnthonyCleopatra      15705  Shakespeare\n",
      "7          HenryVIII      16840  Shakespeare\n",
      "8             HenryV      20633  Shakespeare\n",
      "9         RichardIII      18520  Shakespeare\n",
      "10            Hamlet      18480  Shakespeare\n",
      "initial corpus Mean:\t 21408.75 \t New Corpus Mean:\t 17481.81818181818\n",
      "Corpus median sentence length:\t 8.0\n",
      "Corpus max sentence length:\t 76\n",
      "Shakespeare initial corpus mean  25797 , Shakespeare corpus mean after sampling  18035\n",
      "Marlowe initial sampling         17020 , Marlowe corpus mean after sampling      17020\n"
     ]
    }
   ],
   "source": [
    "corpusStats=wordsPerPlay(projectCorpus)\n",
    "newCorpusStats=wordsPerPlay(NewProjectCorpus)\n",
    "medianSentLength = np.median(NewProjectCorpus['sentence'].apply(lambda x: len([words for words in x.split(\" \") if isinstance(x, str)])))\n",
    "maxSentLength = np.max(NewProjectCorpus['sentence'].apply(lambda x: len([words for words in x.split(\" \") if isinstance(x, str)])))\n",
    "print(\"Initial corpus:\\n\",corpusStats,\"\\n\",\"New Corpus:\\n\",newCorpusStats)\n",
    "print(\"initial corpus Mean:\\t\",corpusStats['Words Sum'].mean(),\"\\t\",\"New Corpus Mean:\\t\",newCorpusStats['Words Sum'].mean())\n",
    "print(\"Corpus median sentence length:\\t\",medianSentLength)\n",
    "print(\"Corpus max sentence length:\\t\",maxSentLength)\n",
    "print(\"Shakespeare initial corpus mean \",int(corpusStats.loc[corpusStats['Author'] == 'Shakespeare']['Words Sum'].mean()),\", Shakespeare corpus mean after sampling \",int(newCorpusStats.loc[corpusStats['Author'] == 'Shakespeare']['Words Sum'].mean()))\n",
    "print(\"Marlowe initial sampling        \",int(corpusStats.loc[corpusStats['Author'] == 'Marlowe']['Words Sum'].mean()),\", Marlowe corpus mean after sampling     \",int(newCorpusStats.loc[corpusStats['Author'] == 'Marlowe']['Words Sum'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing the author with 0 for Marlowe and 1 for Shakespeare\n",
    "NewProjectCorpus['author'].replace(['Marlowe','Shakespeare'],[0,1],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the model and creating the BERT layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT vocab file location : b'C:\\\\Users\\\\Panos\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\d760773f85f64fc84ae0b47310f7cfe3bcec4868\\\\assets\\\\vocab.txt'\n"
     ]
    }
   ],
   "source": [
    "modelURL ='https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'\n",
    "\n",
    "#creating the initial layer\n",
    "bert_layer = hub.KerasLayer(modelURL , trainable=False, name=\"BERT\")\n",
    "#creating the vocabulary file\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "print(f'BERT vocab file location : {vocab_file}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p1><bold>Creating the Tokenizer for BERT</bold></p1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT vocabulary creation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_file):\n",
    "  \"\"\"Loads a vocab file into a list.\"\"\"\n",
    "  vocab = []\n",
    "  with tf.io.gfile.GFile(vocab_file, \"r\") as reader:\n",
    "    while True:\n",
    "      token = reader.readline()\n",
    "      if not token: break\n",
    "      token = token.strip()\n",
    "      vocab.append(token)\n",
    "  return vocab\n",
    "\n",
    "vocab = load_vocab(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabTable(vocab, num_oov=1):\n",
    "  \"\"\"Creates a vocabulary table \"\"\"\n",
    "  vocabValues = tf.range(tf.size(vocab, out_type=tf.int64), dtype=tf.int64)\n",
    "  init = tf.lookup.KeyValueTensorInitializer(keys=vocab, values=vocabValues, key_dtype=tf.string, value_dtype=tf.int64)\n",
    "  vocabTable = tf.lookup.StaticVocabularyTable(init, num_oov, lookup_key_dtype=tf.string)\n",
    "  return vocabTable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabLookupTable = createVocabTable(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createIndex2Word(vocab):\n",
    "  # Create a lookup table for a index to token\n",
    "  vocabValues = tf.range(tf.size(vocab, out_type=tf.int64), dtype=tf.int64)\n",
    "  init = tf.lookup.KeyValueTensorInitializer(keys=vocabValues, values=vocab)\n",
    "  return tf.lookup.StaticHashTable(initializer=init, default_value=tf.constant('unk'), name=\"index2word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.ops.lookup_ops.StaticHashTable at 0x1dccf4fe9d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2Word = createIndex2Word(vocab)\n",
    "index2Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization of the tokenizer with its settings\n",
    "tokenizer = text.BertTokenizer( \n",
    "    vocabLookupTable,\n",
    "    suffix_indicator = \"##\",\n",
    "    max_bytes_per_word = 100,\n",
    "    max_chars_per_token=None,\n",
    "    unknown_token = '[UNK]',\n",
    "    token_out_type=tf.int64,\n",
    "    lower_case=False,\n",
    "    preserve_unused_token = False,    \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8,), dtype=int64, numpy=\n",
       "array([    0,   100,   101,   102,   103, 30522, 30522,  2020],\n",
       "      dtype=int64)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing to see if the vocabulary contains BERT tokens, new line and some words\n",
    "vocabLookupTable.lookup(tf.constant(['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '\\n', 'I', 'were']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID = vocabLookupTable.lookup(tf.constant('[PAD]')) # padding token\n",
    "CLS_ID = vocabLookupTable.lookup(tf.constant('[CLS]')) # class token\n",
    "SEP_ID = vocabLookupTable.lookup(tf.constant('[SEP]')) # sentence separator token\n",
    "def tokenize_text(sentence, seq_len):\n",
    "  # convert text into token ids\n",
    "  tokens = tokenizer.tokenize(sentence)\n",
    "  # flatten the output ragged tensors\n",
    "  tokens = tokens.merge_dims(1, 2)[:, :seq_len]\n",
    "  # Add start and end token ids to the id sequence\n",
    "  startTokens = tf.fill([tf.shape(sentence)[0], 1], CLS_ID)\n",
    "  endTokens = tf.fill([tf.shape(sentence)[0], 1], SEP_ID)\n",
    "  tokens = tokens[:, :seq_len - 2]\n",
    "  tokens = tf.concat([startTokens, tokens, endTokens], axis=1)\n",
    "  # truncate sequences greater than MAX_SEQ_LEN\n",
    "  tokens = tokens[:, :seq_len]\n",
    "  # pad shorter sequences with the pad token id\n",
    "  tokens = tokens.to_tensor(default_value=PAD_ID)\n",
    "  pad = seq_len - tf.shape(tokens)[1]\n",
    "  tokens = tf.pad(tokens, [[0, 0], [0, pad]], constant_values=PAD_ID)\n",
    "\n",
    "  # reshaping the word token ids to fit the output \n",
    "  return tf.reshape(tokens, [-1, seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[CLS]', b'the', b'tragedy', b'of', b'dido', b'queen', b'[SEP]',\n",
       "  b'[PAD]', b'[PAD]']                                              ,\n",
       " [b'[CLS]', b'of', b'carthage', b'.', b'[SEP]', b'[PAD]', b'[PAD]',\n",
       "  b'[PAD]', b'[PAD]', b'[PAD]']                                    ,\n",
       " [b'[CLS]', b'here', b'the', b'curtains', b'draw', b',', b'there', b'is',\n",
       "  b'discovered', b'[SEP]']                                               ,\n",
       " [b'[CLS]', b'ganymede', b'upon', b'his', b'knee', b',', b'and', b'[SEP]'],\n",
       " [b'[CLS]', b'lying', b'asleep', b'.', b'[SEP]', b'[PAD]', b'[PAD]',\n",
       "  b'[PAD]', b'[PAD]', b'[PAD]']                                     ,\n",
       " [b'[CLS]', b'come', b'gentle', b'ganymede', b'and', b'play', b'with',\n",
       "  b'[SEP]']                                                           ,\n",
       " [b'[CLS]', b'i', b'am', b'much', b'better', b'for', b'your', b'worthless',\n",
       "  b'love', b'[SEP]']                                                       ,\n",
       " [b'[CLS]', b'today', b'whenas', b'i', b'filled', b'into', b'your', b'cups',\n",
       "  b'[SEP]']                                                                 ,\n",
       " [b'[CLS]', b'and', b'held', b'the', b'cloth', b'of', b'pleasance',\n",
       "  b'while', b'[SEP]']                                              ,\n",
       " [b'[CLS]', b'as', b'made', b'the', b'blood', b'run', b'down', b'about',\n",
       "  b'mine', b'[SEP]']                                                    ]>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=tokenize_text(NewProjectCorpus['sentence'][0:10].values, 10)\n",
    "tokenizer.detokenize(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessBERTInput(tokens,seq_len):\n",
    "  # calculating the mask\n",
    "  mask = tf.cast(tokens > 0, tf.int64)\n",
    "  mask = tf.reshape(mask, [-1, seq_len])\n",
    "  # calculating token type ID\n",
    "  zeros_dims = tf.stack(tf.shape(mask))\n",
    "  type_ids = tf.fill(zeros_dims, 0)\n",
    "  type_ids = tf.cast(type_ids, tf.int64)\n",
    "  return (mask, type_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingPreProcess(sentence, seq_len):\n",
    "  # calculate token IDs\n",
    "  ids = tokenize_text(sentence, seq_len)\n",
    "  # process review to calculate BERT input\n",
    "  mask, type_ids = preprocessBERTInput(ids,seq_len)\n",
    "  return  ids, mask, type_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcess(sentence, author, seq_len):\n",
    "  # calculate token IDs\n",
    "  ids = tokenize_text(sentence, seq_len)\n",
    "  # process review to calculate BERT input\n",
    "  mask, type_ids = preprocessBERTInput(ids,seq_len)\n",
    "  return  (ids, mask, type_ids),author\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset creation for training, testing and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataset(Dataset,BatchSize,max_seq_len):\n",
    "    \"\"\"Preprocesses the corpus to produce the input for BERT,\n",
    "       creates a batched tensorflow dataset with prefetch \"\"\"\n",
    "    result = PreProcess(Dataset['sentence'].values,Dataset['author'].values,max_seq_len)\n",
    "    result = tf.data.Dataset.from_tensor_slices(result)\n",
    "    result = result.batch(BatchSize)\n",
    "    result = result.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createShuffledDatasets(corpus):\n",
    "    train, validate = train_test_split(corpus, test_size=0.2, shuffle=True,stratify= corpus['author']) \n",
    "    #test, validate = train_test_split(test, test_size=0.5)\n",
    "\n",
    "\n",
    "    trainDataset=createDataset(train,ProjConfig['batch_size'],ProjConfig['max_seq_len'])\n",
    "    EvalDataset=createDataset(validate,ProjConfig['batch_size'],ProjConfig['max_seq_len'])\n",
    "    #testDataset=createDataset(test,ProjConfig['batch_size'],ProjConfig['max_seq_len'])\n",
    "    return trainDataset,EvalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=((TensorSpec(shape=(None, 16), dtype=tf.int64, name=None), TensorSpec(shape=(None, 16), dtype=tf.int64, name=None), TensorSpec(shape=(None, 16), dtype=tf.int64, name=None)), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, validate=createShuffledDatasets(NewProjectCorpus)\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model creation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(maxSeqLen,learningRate,decayFactor):\n",
    "    input_word_ids = tf.keras.layers.Input(shape=(maxSeqLen,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.layers.Input(shape=(maxSeqLen,), dtype=tf.int32, name=\"input_mask\")\n",
    "    input_type_ids = tf.keras.layers.Input(shape=(maxSeqLen,), dtype=tf.int32, name=\"input_type_ids\")\n",
    "    \n",
    "    bert_inputs = dict(\n",
    "        input_word_ids=input_word_ids,\n",
    "        input_mask=input_mask,\n",
    "        input_type_ids=input_type_ids)\n",
    "    #adding the layers\n",
    "    pooled_output = bert_layer(bert_inputs)['pooled_output'] #creates the layer for the three inputs\n",
    "    Dropout = tf.keras.layers.Dropout(0.005, name=\"dropout\")(pooled_output,training=False) #creates the dropout layer, to prevent overfitting (used only during training)\n",
    "    Output = tf.keras.layers.Dense(1, activation='softmax', name=\"output\")(Dropout) # only 2 classes, using sigmoid for activation\n",
    "    #configuring the optimizer\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(\n",
    "        learning_rate=learningRate , #default 0.001\n",
    "        #beta_1=0.9,\n",
    "        #beta_2=0.999,\n",
    "        #epsilon= 1e-4 ,\n",
    "        amsgrad=False,\n",
    "        name='Adam',\n",
    "        decay=decayFactor\n",
    "    )\n",
    "\n",
    "    model = tf.keras.Model(inputs=bert_inputs, outputs = Output)\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_mask (InputLayer)        [(None, 16)]         0           []                               \n",
      "                                                                                                  \n",
      " input_type_ids (InputLayer)    [(None, 16)]         0           []                               \n",
      "                                                                                                  \n",
      " input_word_ids (InputLayer)    [(None, 16)]         0           []                               \n",
      "                                                                                                  \n",
      " BERT (KerasLayer)              {'default': (None,   109482241   ['input_mask[0][0]',             \n",
      "                                768),                             'input_type_ids[0][0]',         \n",
      "                                 'encoder_outputs':               'input_word_ids[0][0]']         \n",
      "                                 [(None, 16, 768),                                                \n",
      "                                 (None, 16, 768),                                                 \n",
      "                                 (None, 16, 768),                                                 \n",
      "                                 (None, 16, 768),                                                 \n",
      "                                 (None, 16, 768),                                                 \n",
      "                                 (None, 16, 768),                                                 \n",
      "                                 (None, 16, 768),                                                 \n",
      "                                 (None, 16, 768),                                                 \n",
      "                                 (None, 16, 768),                                                 \n",
      "                                 (None, 16, 768),                                                 \n",
      "                                 (None, 16, 768),                                                 \n",
      "                                 (None, 16, 768)],                                                \n",
      "                                 'pooled_output': (                                               \n",
      "                                None, 768),                                                       \n",
      "                                 'sequence_output':                                               \n",
      "                                 (None, 16, 768)}                                                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 768)          0           ['BERT[0][13]']                  \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1)            769         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,483,010\n",
      "Trainable params: 769\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model=createModel(ProjConfig['max_seq_len'],ProjConfig['learning_rate'],ProjConfig['decay_factor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel (epochsNumber,cycles):\n",
    "    history=pd.DataFrame()\n",
    "    for cycle in range(cycles):\n",
    "     \n",
    "        print(f'Cycle number: {cycle+1}')\n",
    "        trainDataset,EvalDataset=createShuffledDatasets(NewProjectCorpus)\n",
    "\n",
    "        checkpoint_path = \"c:\\\\model\\\\\"\n",
    "        checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "        # Create a callback that saves the model's weights\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                                save_weights_only=True,\n",
    "                                                                verbose=1)\n",
    "\n",
    "        #load the model from the checkpoint path\n",
    "        #model.load_weights(checkpoint_path)\n",
    "\n",
    "        #Tuning decay via Learning rate \n",
    "        def TuneLearningRate(epochsNumber,rate):\n",
    "            if epoch < 6 :#or cycle < 0 :\n",
    "                return rate\n",
    "            else:\n",
    "                return rate * tf.math.exp(-0.0001)\n",
    "\n",
    "        #create callbcak for the learning rate scheduler\n",
    "        scheduler = tf.keras.callbacks.LearningRateScheduler(TuneLearningRate)\n",
    "\n",
    "        results=model.fit(trainDataset,\n",
    "                    epochs=epochsNumber,\n",
    "                    validation_data=EvalDataset,\n",
    "                    #callbacks=[checkpoint_callback,scheduler]\n",
    "                    callbacks=[checkpoint_callback]\n",
    "                    )\n",
    "        history=history.append(results.history, ignore_index=True)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle number: 1\n",
      "Epoch 1/6\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.6898 - accuracy: 0.4793\n",
      "Epoch 1: saving model to c:\\model\\\n"
     ]
    }
   ],
   "source": [
    "results=trainModel(ProjConfig['epochs'], ProjConfig['cycles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotResults(results):\n",
    "    acc = results['accuracy']\n",
    "    val_acc = results['val_accuracy']\n",
    "    loss = results['loss']\n",
    "    val_loss = results['val_loss']\n",
    "\n",
    "    epochs_range = range(ProjConfig['epochs'])\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy',)\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.suptitle('Learning rate: '+ str((ProjConfig['learning_rate']))+'\\n'\n",
    "                'Decay factor: '+ str((ProjConfig['decay_factor']))+'\\n'\n",
    "                'Max Sequence Length: '+ str((ProjConfig['max_seq_len']))+'\\n' \n",
    "                'cycle: '+ str((results.name))+'\\n' +'\\n')\n",
    "                \n",
    "   \n",
    "    plt.title('\\n' +'Training and Validation Accuracy')\n",
    "    plt.subplot(1, 2, 2)\n",
    "\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lists = results[['loss','accuracy','val_loss','val_accuracy']].unstack().apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lists[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cycle in results_lists.columns:\n",
    "    plotResults(results_lists[cycle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions =model.predict(testDataset)\n",
    "PredictionsList=[]\n",
    "PredictionsList.append(Predictions)\n",
    "\n",
    "testPredictions=test\n",
    "testPredictions['predictions']=PredictionsList[0]\n",
    "testPredictions['predictionsRounded']=testPredictions['predictions'].apply(round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_likelihood_ratios(testPredictions['author'],testPredictions['predictionsRounded'],labels=(0,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredictions.loc[testPredictions['author'] == testPredictions['predictionsRounded']].play.value_counts()#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredictions.loc[testPredictions['author'] != testPredictions['predictionsRounded']].play.value_counts()#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanShakespeareText(filePath):\n",
    "    with open(filePath) as file:\n",
    "        lines = file.readlines()\n",
    "        file.close()\n",
    "    text=[]\n",
    "\n",
    "    for line in lines:\n",
    "        if line =='\\n':\n",
    "            continue\n",
    "        \n",
    "        line=clean.removeSpeakerName(line) #remove speaker's name from  each line\n",
    "        line=clean.removeLinesBasedOnWords(['ACT', 'SCENE'],line) # removing lines with the words ACTS and Scenes\n",
    "        line=clean.removeWords(['1','2','3','4','5','6','7','8','9'],line)\n",
    "        line=clean.removeLinesBasedOnWords(['Exit'],line)\n",
    "        line=clean.removeLinesBasedOnWords(['Enter'],line)\n",
    "        line=clean.remove_words_in_brackets(line)\n",
    "        line=clean.removeWhitespace(line)\n",
    "        text.append(line)\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanFilesinPath(playPath,cleanFilesPath):\n",
    "    for play in os.scandir(playPath):\n",
    "        if play.is_file():\n",
    "            print((\" cleaning  \") + (play.name))\n",
    "            cleanedPlay=cleanShakespeareText(play.path)\n",
    "            \n",
    "            with open(cleanFilesPath+play.name.removesuffix('.txt')+'Cleaned'+'.txt', 'w') as cleanedFile:\n",
    "                cleanedFile.write(\"\\n\".join(str(item) for item in cleanedPlay))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OriginalShakespearePath='./Corpus/UnseenTexts/Shakespeare/'\n",
    "CleanShakespearePath='./Corpus/UnseenTexts/Shakespeare/CleanedPlays/'\n",
    "cleanFilesinPath(OriginalShakespearePath,CleanShakespearePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importPlaysinCorpus(CorpusPath,Author):\n",
    "    corpus=pd.DataFrame()\n",
    "    for play in os.scandir(CorpusPath):\n",
    "        if play.is_file():\n",
    "            print((\" importing  \") + (play.path) + (\" to corpus\"))\n",
    "            newplay=pd.read_csv(play.path, delimiter='\\r', header=None, names=['sentence', 'author', 'play'])\n",
    "            newplay[['author']]=Author\n",
    "            if 'Cleaned' in play.path:\n",
    "                newplay[['play']]=os.path.basename(play.path).removesuffix('Cleaned.txt')\n",
    "            else:\n",
    "                newplay[['play']]=os.path.basename(play.path).removesuffix('.txt')\n",
    "            corpus=pd.concat([corpus, newplay], axis = 0,join='outer')\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ShakespeareUnseenPath='./Corpus/UnseenTexts/Shakespeare/CleanedPlays/'\n",
    "MarloweUnseenPath='./Corpus/UnseenTexts/Marlowe/'\n",
    "ShakespeareUnseen=pd.DataFrame()\n",
    "\n",
    "ShakespeareUnseen=importPlaysinCorpus(ShakespeareUnseenPath,'Shakespeare')\n",
    "ShakespeareUnseen['sentence']=ShakespeareUnseen['sentence'].str.lower()\n",
    "\n",
    "MarloweUnseen=importPlaysinCorpus(MarloweUnseenPath,'Marlowe')\n",
    "MarloweUnseen['sentence']=MarloweUnseen['sentence'].str.lower()\n",
    "Unseen=pd.DataFrame()\n",
    "Unseen=pd.concat([Unseen,MarloweUnseen],axis=0, join=\"outer\")\n",
    "Unseen=pd.concat([Unseen,ShakespeareUnseen],axis=0, join=\"outer\")\n",
    "#shuffling line in the dataframe\n",
    "Unseen=Unseen.sample(frac = 1)\n",
    "#replacing the author with 0 for Marlowe and 1 for Shakespeare\n",
    "Unseen['author'].replace(['Marlowe','Shakespeare'],[0,1],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "UnseenDataset=createDataset(Unseen,ProjConfig['batch_size'],ProjConfig['max_seq_len'])\n",
    "Predictions =model.predict(UnseenDataset)\n",
    "PredictionsList=[]\n",
    "PredictionsList.append(Predictions)\n",
    "\n",
    "\n",
    "Unseen['predictions']=PredictionsList[0]\n",
    "Unseen['predictionsRounded']=Unseen['predictions'].apply(round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictionsStats=[]\n",
    "PredictionsStats=Unseen.loc[Unseen['author'] != Unseen['predictionsRounded']].play.value_counts()\n",
    "PredictionsStats.columns=['author','Not Correct']\n",
    "PredictionsStats=PredictionsStats.to_frame(name='Not Correct')\n",
    "PredictionsStats['Correct']=Unseen.loc[Unseen['author'] == Unseen['predictionsRounded']].play.value_counts()\n",
    "PredictionsStats['Accuracy']=PredictionsStats['Correct']/(PredictionsStats['Correct']+PredictionsStats['Not Correct'])\n",
    "PredictionsStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_likelihood_ratios(Unseen['author'],Unseen['predictionsRounded'],labels=(0,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Marlowe-TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
